---
title: "Are GPTs capped at human intelligence?"
date: "2023-10-08T10:012:44.675Z"
description: "Thoughts on the claim that transforms doing next token prediction cannot surpass human performance"
thumbnail: "/img/blog/thumbnail1.png"
---


Some claim that there is an implicit upper bound on the capabilities of Large Language Models, because they are trained to predict the next token in datasets 'generated by humans'. Their argument is sometimes based on information theory; GPTs* are inherently limited to the information contained within the training set. This means that they cannot generate new information or knowledge that is not already present in the training data, which is, in their view, a low resolution subset of human knowledge.

Given that the dataset is supposedly human-generated and limited in size (the argument goes), the language model cannot surpass human performance.

I think that this argument is flawed, for a number of reasons, which I will explain below.

\* I use GPTs to refer to transformers trained on next token prediction

### 1. The training data is not limited to human-generated text

The datasets used to train large language models contain decidedly non-human data. Alongside novels, articles and tweets, the dataset contains historic time series data, weather patterns, machine generated log lines, stock prices, medical data and more.

An optimizer attempting to minimize loss on such a dataset will be forced to attempt to approximate the underlying data-generating processes. This means that a language model is not a model of human language, or even of a language-generating human, but a model of the world.

Consider the fact that somewhere in the training data there exists the result of a hash function, followed by the string of characters that were hashed to produce that result. If the language model is to predict the sequence of tokens that follow the hash function, it must crack the hash function. This is not to say that any model would be able to crack a modern hash function; the point here is that there are text-generating processes that have contributed to the training data on which models have been trained, that far exceed natural language in their complexity. 


### 2. LLM architecture is not the be all and end all of artificial intelligence

[Attention Is All You Need](https://arxiv.org/abs/1706.03762) introduced us to the transformer in 2017. In the intervening years, the transformer has been responsible for many of the critical breakthroughs we have seen in AI. It is a sufficient architecture for many tasks, and its limits are not yet understood; architectures very similar to those that we have today could continue to deliver more general and capable systems. But the transformer, and the implementation details of our current architectures, are not necessarily the end of the road.

We could find that the design of the most capable models we have today are proveably capped in their capabilities, but that does not mean that large language models are a dead end. There will be other roads to explore, and other architectures to try.

A [quote from Gwern Branwen](https://gwern.net/scaling-hypothesis) sums this thought up well:

_"This year, GPT-3 is scary because it’s a magnificently obsolete architecture from early 2018 (used mostly for software engineering convenience as the infrastructure has been debugged), which is small & shallow compared to what’s possible, with a simple uniform architecture trained in the dumbest way possible (unidirectional prediction of next text token) on a single impoverished modality (random Internet HTML text dumps) on tiny data (fits on a laptop), sampled in a dumb way8, its benchmark performance sabotaged by bad prompts & data encoding problems (especially arithmetic & commonsense reasoning), and yet, the first version already manifests crazy runtime meta-learning—and the scaling curves still are not bending!"_

### 3. The Scaling curves are not bending 

Transformer-based language models can be of varying parameter count, and they can be trained for different amounts of time on different amounts of data.

- Parameter count: as the complexity of the model increases, so does its ability to model more complex functions; in the same way that a linear equation can only plot a straight line on a graph but a cubic polynomial can be used to approximate slightly more complex data, so too can a model with more parameters more closely approximate the complexities of reality.
- Training data: the more data a model is given to train on, the more information it has to learn about the world, and the more patterns exist latent in the data hat the neural network can learn so that it can make better predictions at test time.
- Compute: training the model for longer allows the model more time to learn the training data, similarly to how a student with more time to revise for an exam can be expected to outperform a student given less time. 

Researchers have trained thousands of models using different combinations of these 3 values. They vary the amount of training data, the size of the model, and the FLOPS used to train the model, and then judge the resulting model on benchmarks to test how capable it is at a range of tasks. 

It turns out, that the capabilities of the resulting model are not random. The models ability on a given benchmark is a function of it's parameter count, training compute and training set. 

This was a huge revelation; it meant that we can predict the abilities of models in advance of training them (Note: we can't predict emergent capabilities, i.e. we could not predict in advance that models would be able to follow instructions given in their prompt, for example.)

The critical point here is that the curves are not bending. Although we suspect that models would plateau eventually, we have not seen good evidence to suggest that this is the case.

The limit is currently a question of finding sufficient compute and training data, given that GPUs are in short supply and the corpus of human-generated data has been all but exhausted. 

If we can get around these issues i.e. by finding ways to use synthetic data that don't result in mode collapse, we can expect our model perfroamnce to continue advancing up and to the right. 


### 4. LLMs are simulating every underlying data-generating processes simultaneously

[Simulator theory](https://generative.ink/posts/simulators/) suggests that LLMs can't be viewed under traditional ontologies; they're not agents or orales or otherwise. Instead, they are best conceived as simulators.

I have not come across a more complete or convincing way of describing what it is that LMs do. 

This is relevant because if it is true, it means that what language models are doing is vastly different to the computation that the human brain is doing. 

It means that achieving optimal loss is not a question of approximating the average human being, it instead requires approximating the base reality that brought about all text generating processes that contributed to the training data.

This makes GPTs a fundamentally different type of intelligence to human intelligenc. It makes us incomparable, and the question of our intelligence in comparison to that of GPTs a moot point.
