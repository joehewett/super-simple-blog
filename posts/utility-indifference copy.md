---
title: 'Are LLMs capped at human intelligence?'
date: '2023-10-02T18:07:44.675Z'
description: 'Thoughts on the claim that next-token prediction cannot surpass human performance'
thumbnail: '/img/blog/thumbnail12.png'
---

Some people claim that there is an implicit upper bound on the capabilities of Large Language Models, because of the fact that they are trained to predict the next token in a sequence of data generated by a human. Their argument is sometimes based on information theory, namely that since language models are trained by optimizing their ability to predict the next token in a sequence of tokens, their capabilities are inherently limited to the information contained within the training set. This means that they cannot generate new information or knowledge that is not already present in the training data.

Given that the dataset is supposedly human-generated and limited in size (the argument goes), the language model cannot surpass human performance.

I think that this argument is flawed, for a number of reasons.

### 1. The training data is not limited to human-generated text
TODO

### 2. LLMs can be trained using arbitrary amounts of compute and data
TODO

### 3. LLMs are simulating every underlying data-generating processes simultaneously
TODO
