---
title: 'LLM Bad Takes'
date: '2023-10-15T00:00:00.000Z'
description: 'Rebuttals to counter claims made against the capabilities of Language Models'
thumbnail: '/img/blog/thumbnail17.png'
---

### WIP: These are rough notes for a talk I am giving in November. They are not complete yet.


### Plan

In this essay we will talk about some bad arguments:

* 1) GPTs are only trying to predict the next token
  * Counter with analogy with jinclusive genetic fitness
  * We can understand the base objective only, we can’t say for certainty anything beyond that
* 2) GPTs won’t be deceptive because SGD will converge to simplest solution, i.e. average of text producing processes
  * Counter with the fact that LLMs are not emulating the average person, they’re emulating every process that contributed to the training data
* 3) GPTs can only be as smart as the smartest human since its trained on human data
  * Counter with the fact that non-human processes contribute to the training data 
    * e.g. the hash problem dskl5236GDFG235vdGERb43yeby33b = “my string” 
* 4) GPTs are just stochastic parrots, the text generated is just the next most probably token 
  * Counter with the fact that we’re not concerned by WHAT the model is doing, but HOW it is doing it
* 5) Bonus bad argument: “Yeah but GPT-n can’t do X”
  * Point at the speed at which LLMs are breaking benchmarks
* 6) GPTs can't reason 
  * https://arxiv.org/pdf/2308.03762.pdf      