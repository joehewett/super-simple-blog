---
title: 'Are LLMs capped at human intelligence?'
date: '2023-10-02T18:07:44.675Z'
description: 'Thoughts on the claim that next-token prediction cannot surpass human performance'
thumbnail: '/img/blog/thumbnail12.png'
---

Some people claim that there is an implicit upper bound on the capabilities of Large Language Models, because of the fact that they are trained to predict the next token in a sequence of data generated by a human. Their argument is sometimes based on information theory, namely that since language models are trained by optimizing their ability to predict the next token in a sequence of tokens, their capabilities are inherently limited to the information contained within the training set. This means that they cannot generate new information or knowledge that is not already present in the training data.

Given that the dataset is supposedly human-generated and limited in size (the argument goes), the language model cannot surpass human performance.

I think that this argument is flawed, for a number of reasons.

### 1. The training data is not limited to human-generated text
The datasets used to train large language models contain decidedly non-human data. Alongside novels, articles and tweets, the dataset contains historic time series data, weather patterns, machine generated log lines, stock prices, medical data and more. 

An optimizer attempting to minimize loss on such a dataset will be forced to attempt to approximate the underlying data-generating processes. This means that a language model is not a model of human language, or even of a language-generating human, but a model of the world.

Consider the fact that somewhere in the training data there exists the result of a hash function, followed by the string of characters that were hashed to produce that result. If the language model is to predict the sequence of tokens that follow the hash function, it must crack the hash function. This is a non-trivial task well beyond the capabilities of humans, and yet language models are, in theory, capable of doing it.

### 2. LLM architecture is not the be all and end all of artificial intelligence
A quote from Gwern Branwen sums this thought up well: 

_"This year, GPT-3 is scary because it’s a magnificently obsolete architecture from early 2018 (used mostly for software engineering convenience as the infrastructure has been debugged), which is small & shallow compared to what’s possible, with a simple uniform architecture trained in the dumbest way possible (unidirectional prediction of next text token) on a single impoverished modality (random Internet HTML text dumps) on tiny data (fits on a laptop), sampled in a dumb way8, its benchmark performance sabotaged by bad prompts & data encoding problems (especially arithmetic & commonsense reasoning), and yet, the first version already manifests crazy runtime meta-learning—and the scaling curves still are not bending!"_


### 3. The Scaling Hypothesis is holding
TODO

### 4. LLMs can be trained using arbitrary amounts of compute and data
TODO

### 5. LLMs are simulating every underlying data-generating processes simultaneously
TODO
